ppp
cairo_pdf(file.path(figures_path,'Figure_trialT0.pdf'))
print(ppp)
dev.off()
info <- read.delim("~/Desktop/SwitchDrive/OBIWAN/Fichiers rÃ©sultats NORA/info_expe.txt", header = T, sep ='') # read in dataset
info$Subject = info$id
# Preprocess Sess 1--------------------------------------------------------------
data  <- subset(full, Session == 1) #subset #only session one
data  <- subset(data, Group == 'O') #subset #only group )
data  <- subset(data, Phase == 'proc1') #only learning phase
#recode participants
for (i in  1:length(data$Subject)) {
if(data$Subject[i] == 2251) #fix the bug
{data$Subject[i] = 245}
else if(data$Subject[i] == 2512210)
{data$Subject[i] = 251}
else if(data$Subject[i] > 2000)
{data$Subject[i] = data$Subject[i] - 2000}
else
{data$Subject[i] = data$Subject[i]}
}
data = merge(data, info, by = "Subject")
#factorize and rename
data$type = as.factor(revalue(data$imcor, c(A="AB", C="CD", E="EF")))
data$reward = revalue(data$feedback, c(Negatif=0, Positif=1))
data$side = revalue(data$Stim.RESP, c(x='L', n='R'))
data$subjID = data$Subject
#This loop is there to transform everything into one column "choice"
#this column takes a 1 if the action was to choose either A, C or E
#and takes 0 if the response is either B, D or F (and that independently of the side)
data$choice = c(1:length(data$Trial)) #initialize variable
for (i in  1:length(data$Trial)) {
if((data$side[i] == 'L')&(data$img[i] == 'A' || data$img[i] == 'C' || data$img[i] == 'E'))
{data$choice[i] = 1}
else if ((data$side[1] == 'R')&(data$imd[i] == 'A' || data$imd[i] == 'C' || data$imd[i] == 'E'))
{data$choice[i] = 1}
else
{data$choice[i] = 0}
}
data$reward = as.numeric(data$reward)
data$type = revalue(data$type, c(AB=12, CD=34, EF=56))
data$type = as.numeric(as.character(data$type))
bs = ddply(data, .(Subject, imcor), summarise, acc = mean(Stim.ACC, na.rm = TRUE))
# Crtierium chose A at 65%, C at 60% and E at 50% and min 30 trials.
bs_wide <- spread(bs, imcor, acc)
bs_wide$pass = c(1:length(bs_wide$Subject)) #initialize variable
for (i in  1:length(bs_wide$Subject)) {
if((bs_wide$A[i] >= 0.65) && (bs_wide$C[i] >=  0.60) && (bs_wide$E[i] >= 0.50 ))
{bs_wide$pass[i] = 1}
else {bs_wide$pass[i] = 0}
}
data = merge(data, bs_wide[ , c("Subject", "pass")], by = "Subject", all.x=TRUE)
data = subset(data, pass == 1)
dataclean <- select(data, c(subjID, type, choice, reward, intervention))
count_trial = dataclean %>% group_by(subjID, type) %>%tally()
# Time to acquire reach criterium -----------------------------------------------------
dataclean$count = 1
bs1 = dataclean %>% ddply(.(subjID, intervention), summarise, n = sum(count))
densityPlot(bs1$n)
bs1$Session =1
# Preprocess Sess 2--------------------------------------------------------------
data  <- subset(full, Session == 2) #subset #only session one
data  <- subset(data, group == 'O') #subset #only session one
data  <- subset(data, Phase == 'proc1') #only learning phase
#recode participants
for (i in  1:length(data$Subject)) {
if(data$Subject[i] == 2251) #fix the bug
{data$Subject[i] = 245}
else if(data$Subject[i] == 2512210)
{data$Subject[i] = 251}
else if(data$Subject[i] > 2000)
{data$Subject[i] = data$Subject[i] - 2000}
else
{data$Subject[i] = data$Subject[i]}
}
data = merge(data, info, by = "Subject")
#factorize and rename
data$type = as.factor(revalue(data$imcor, c(A="AB", C="CD", E="EF")))
data$reward = revalue(data$feedback, c(Negatif=0, Positif=1))
data$side = revalue(data$Stim.RESP, c(x='L', n='R'))
data$subjID = data$Subject
#This loop is there to transform everything into one column "choice"
#this column takes a 1 if the action was to choose either A, C or E
#and takes 0 if the response is either B, D or F (and that independently of the side)
data$choice = c(1:length(data$Trial)) #initialize variable
for (i in  1:length(data$Trial)) {
if((data$side[i] == 'L')&(data$img[i] == 'A' || data$img[i] == 'C' || data$img[i] == 'E'))
{data$choice[i] = 1}
else if ((data$side[1] == 'R')&(data$imd[i] == 'A' || data$imd[i] == 'C' || data$imd[i] == 'E'))
{data$choice[i] = 1}
else
{data$choice[i] = 0}
}
data$reward = as.numeric(data$reward)
data$type = revalue(data$type, c(AB=12, CD=34, EF=56))
data$type = as.numeric(as.character(data$type))
bs = ddply(data, .(Subject, imcor), summarise, acc = mean(Stim.ACC, na.rm = TRUE))
# Crtierium chose A at 65%, C at 60% and E at 50% and min 30 trials.
bs_wide <- spread(bs, imcor, acc)
bs_wide$pass = c(1:length(bs_wide$Subject)) #initialize variable
for (i in  1:length(bs_wide$Subject)) {
if((bs_wide$A[i] >= 0.65) && (bs_wide$C[i] >=  0.60) && (bs_wide$E[i] >= 0.50 ))
{bs_wide$pass[i] = 1}
else {bs_wide$pass[i] = 0}
}
data = merge(data, bs_wide[ , c("Subject", "pass")], by = "Subject", all.x=TRUE)
data = subset(data, pass == 1)
dataclean <- select(data, c(subjID, type, choice, reward, intervention))
count_trial = dataclean %>% group_by(subjID, type) %>%tally()
# Time to acquire reach criterium -----------------------------------------------------
dataclean$count = 1
bs2 = dataclean %>% ddply(.(subjID, intervention), summarise, n = sum(count))
densityPlot(bs2$n)
bs2$Session = 2
df = rbind(bs1, bs2)
View(df)
fac <- c("subjID", "Session", "intervention")
df[fac] <- lapply(df[fac], factor)
BF <- anovaBF(n ~ Session*intervention  + subjID, data = df,
whichRandom = "subjID", iterations = 50000)
BF; plot(BF)
??gather
frqaov <- aov_car(n ~ Session*intervention + Error (subjID/Session), data = df, anova_table = list(correction = "GG", es = "pes"))
frqaov
pacman::p_load(tidyverse, plyr,dplyr,readr, car, BayesFactor, sjmisc, effectsize, afex) #whatchout to have tidyBF 0.3.0
frqaov <- aov_car(n ~ Session*intervention + Error (subjID/Session), data = df, anova_table = list(correction = "GG", es = "pes"))
frqaov
pal= "#21908CFF" # add color
pal[2] = "black" # add one
averaged_theme <- theme_bw(base_size = 32, base_family = "Helvetica")+
theme(strip.text.x = element_text(size = 32, face = "bold"),
strip.background = element_rect(color="white", fill="white", linetype="solid"),
legend.position=c(.9,.9),
plot.subtitle  = element_text(size = 14),
legend.text  = element_text(size = 10),
legend.key = element_rect(fill = "transparent", colour = "transparent"),
panel.grid.major.x = element_blank() ,
panel.grid.major.y = element_line(size=.2, color="lightgrey") ,
panel.grid.minor = element_blank(),
axis.title.x = element_text(size = 30),
axis.title.y = element_text(size =  30),
axis.line = element_line(size = 0.5),
panel.border = element_blank())
mod <- lmerTest::lmer(n ~ Session*intervention + (1|subjID) + (1|Session), data = df)
plot = interactions::cat_plot(mod, pred = Session, modx = intervention, geom = "bar", interval = T, plot.points = TRUE)
ppp = plot + labs(subtitle = "", x ='', y ='Trials to achieve criterion') +
scale_color_manual(name = "", labels=c("Placebo", "Liraglutide"), values=c("0" = pal[2],"1"=pal[1])) +
scale_fill_manual(name = "", labels=c("Placebo", "Liraglutide"), values=c("0" = pal[2],"1"=pal[1])) +
#scale_y_continuous(expand = c(0, 0), breaks = c(seq.int(0,3500, by = 500)), limits = c(-100, 3500)) +
scale_x_discrete(labels=c("Pre", "Post")) + theme_bw() + averaged_theme
ppp
cairo_pdf(file.path(figures_path,'Figure_trialLIRA.pdf'))
print(ppp)
dev.off()
## R code for FOR PROBA LEARNING TASK OBIWAN
# last modified on April 2020 by David MUNOZ TORD
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE))
# PRELIMINARY STUFF ----------------------------------------
if(!require(pacman)) {
install.packages("pacman", "devtools")
library(devtools)
library(pacman)
}
pacman::p_load(tidyverse, plyr,dplyr,readr, car, BayesFactor, sjmisc, parallel, effectsize, pracma) #whatchout to have tidyBF 0.3.0
options(mc.cores = parallel::detectCores()) #to mulithread
# SETUP ------------------------------------------------------------------
task = 'PBlearning'
# Set working directory #change here if the switchdrive is not on your home folder
analysis_path <- file.path('~/Desktop/SwitchDrive/OBIWAN/PROBA_LEARNING/Analysis')
setwd(analysis_path)
# open dataset
full <- read_csv("~/Desktop/SwitchDrive/OBIWAN/PROBA_LEARNING/PBLearning.csv")
#load("~/OBIWAN/DERIVATIVES/BEHAV/PBL_OBIWAN_T0.RData") # if you dont want ot recompute and go directly to stats
# Preprocess --------------------------------------------------------------
data  <- subset(full, Session == 1) #subset #only session one
data  <- subset(data, Phase == 'proc1') #only learning phase
#factorize and rename
data$type = as.factor(revalue(data$imcor, c(A="AB", C="CD", E="EF")))
data$reward = revalue(data$feedback, c(Negatif=0, Positif=1))
data$side = revalue(data$Stim.RESP, c(x='L', n='R'))
data$subjID = data$Subject
#This loop is there to transform everything into one column "choice"
#this column takes a 1 if the action was to choose either A, C or E
#and takes 0 if the response is either B, D or F (and that independently of the side)
data$choice = c(1:length(data$Trial)) #initialize variable
for (i in  1:length(data$Trial)) {
if((data$side[i] == 'L')&(data$img[i] == 'A' || data$img[i] == 'C' || data$img[i] == 'E')) {
data$choice[i] = 1
} else if ((data$side[1] == 'R')&(data$imd[i] == 'A' || data$imd[i] == 'C' || data$imd[i] == 'E')) {
data$choice[i] = 1}
else {
data$choice[i] = 0}
}
data$reward = as.numeric(data$reward)
data$type = revalue(data$type, c(AB=12, CD=34, EF=56))
data$type = as.numeric(as.character(data$type))
bs = ddply(data, .(Subject, imcor), summarise, acc = mean(Stim.ACC, na.rm = TRUE))
# Crtierium chose A at 65%, C at 60% and E at 50% and min 30 trials.
bs_wide <- spread(bs, imcor, acc)
bs_wide$pass = c(1:length(bs_wide$Subject)) #initialize variable
for (i in  1:length(bs_wide$Subject)) {
if((bs_wide$A[i] >= 0.65) && (bs_wide$C[i] >=  0.60) && (bs_wide$E[i] >= 0.50 ))
{bs_wide$pass[i] = 1}
else {bs_wide$pass[i] = 0}
}
## R code for FOR PROBA LEARNING TASK OBIWAN
# last modified on April 2020 by David MUNOZ TORD
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE))
# PRELIMINARY STUFF ----------------------------------------
if(!require(pacman)) {
install.packages("pacman", "devtools")
library(devtools)
library(pacman)
}
pacman::p_load(tidyverse, plyr,dplyr,readr, car, BayesFactor, sjmisc, parallel, effectsize, pracma) #whatchout to have tidyBF 0.3.0
options(mc.cores = parallel::detectCores()) #to mulithread
# SETUP ------------------------------------------------------------------
task = 'PBlearning'
# Set working directory #change here if the switchdrive is not on your home folder
analysis_path <- file.path('~/Desktop/SwitchDrive/OBIWAN/PROBA_LEARNING/Analysis')
setwd(analysis_path)
# open dataset
full <- read_csv("~/Desktop/SwitchDrive/OBIWAN/PROBA_LEARNING/PBLearning.csv")
#load("~/OBIWAN/DERIVATIVES/BEHAV/PBL_OBIWAN_T0.RData") # if you dont want ot recompute and go directly to stats
# Preprocess --------------------------------------------------------------
data  <- subset(full, Session == 1) #subset #only session one
data  <- subset(data, Phase == 'proc1') #only learning phase
#factorize and rename
data$type = as.factor(revalue(data$imcor, c(A="AB", C="CD", E="EF")))
data$reward = revalue(data$feedback, c(Negatif=0, Positif=1))
data$side = revalue(data$Stim.RESP, c(x='L', n='R'))
data$subjID = data$Subject
#This loop is there to transform everything into one column "choice"
#this column takes a 1 if the action was to choose either A, C or E
#and takes 0 if the response is either B, D or F (and that independently of the side)
data$choice = c(1:length(data$Trial)) #initialize variable
for (i in  1:length(data$Trial)) {
if((data$side[i] == 'L')&(data$img[i] == 'A' || data$img[i] == 'C' || data$img[i] == 'E')) {
data$choice[i] = 1
} else if ((data$side[1] == 'R')&(data$imd[i] == 'A' || data$imd[i] == 'C' || data$imd[i] == 'E')) {
data$choice[i] = 1}
else {
data$choice[i] = 0}
}
data$reward = as.numeric(data$reward)
data$type = revalue(data$type, c(AB=12, CD=34, EF=56))
data$type = as.numeric(as.character(data$type))
bs = ddply(data, .(Subject, imcor), summarise, acc = mean(Stim.ACC, na.rm = TRUE))
# Crtierium chose A at 65%, C at 60% and E at 50% and min 30 trials.
bs_wide <- spread(bs, imcor, acc)
bs_wide$pass = c(1:length(bs_wide$Subject)) #initialize variable
for (i in  1:length(bs_wide$Subject)) {
if((bs_wide$A[i] >= 0.65) && (bs_wide$C[i] >=  0.60) && (bs_wide$E[i] >= 0.50 ))
{bs_wide$pass[i] = 1}
else {bs_wide$pass[i] = 0}
}
data$Subject
unique(data$Subject)
length(unique(data$Subject))
x =subset(data, pass == 1)
## R code for FOR PROBA LEARNING TASK OBIWAN
# last modified on April 2020 by David MUNOZ TORD
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE))
# PRELIMINARY STUFF ----------------------------------------
if(!require(pacman)) {
install.packages("pacman", "devtools")
library(devtools)
library(pacman)
}
pacman::p_load(tidyverse, plyr,dplyr,readr, car, BayesFactor, sjmisc, parallel, effectsize, pracma) #whatchout to have tidyBF 0.3.0
options(mc.cores = parallel::detectCores()) #to mulithread
# SETUP ------------------------------------------------------------------
task = 'PBlearning'
# Set working directory #change here if the switchdrive is not on your home folder
analysis_path <- file.path('~/Desktop/SwitchDrive/OBIWAN/PROBA_LEARNING/Analysis')
setwd(analysis_path)
# open dataset
full <- read_csv("~/Desktop/SwitchDrive/OBIWAN/PROBA_LEARNING/PBLearning.csv")
#load("~/OBIWAN/DERIVATIVES/BEHAV/PBL_OBIWAN_T0.RData") # if you dont want ot recompute and go directly to stats
# Preprocess --------------------------------------------------------------
data  <- subset(full, Session == 1) #subset #only session one
data  <- subset(data, Phase == 'proc1') #only learning phase
#factorize and rename
data$type = as.factor(revalue(data$imcor, c(A="AB", C="CD", E="EF")))
data$reward = revalue(data$feedback, c(Negatif=0, Positif=1))
data$side = revalue(data$Stim.RESP, c(x='L', n='R'))
data$subjID = data$Subject
#This loop is there to transform everything into one column "choice"
#this column takes a 1 if the action was to choose either A, C or E
#and takes 0 if the response is either B, D or F (and that independently of the side)
data$choice = c(1:length(data$Trial)) #initialize variable
for (i in  1:length(data$Trial)) {
if((data$side[i] == 'L')&(data$img[i] == 'A' || data$img[i] == 'C' || data$img[i] == 'E')) {
data$choice[i] = 1
} else if ((data$side[1] == 'R')&(data$imd[i] == 'A' || data$imd[i] == 'C' || data$imd[i] == 'E')) {
data$choice[i] = 1}
else {
data$choice[i] = 0}
}
data$reward = as.numeric(data$reward)
data$type = revalue(data$type, c(AB=12, CD=34, EF=56))
data$type = as.numeric(as.character(data$type))
bs = ddply(data, .(Subject, imcor), summarise, acc = mean(Stim.ACC, na.rm = TRUE))
# Crtierium chose A at 65%, C at 60% and E at 50% and min 30 trials.
bs_wide <- spread(bs, imcor, acc)
bs_wide$pass = c(1:length(bs_wide$Subject)) #initialize variable
for (i in  1:length(bs_wide$Subject)) {
if((bs_wide$A[i] >= 0.65) && (bs_wide$C[i] >=  0.60) && (bs_wide$E[i] >= 0.50 ))
{bs_wide$pass[i] = 1}
else {bs_wide$pass[i] = 0}
}
data = merge(data, bs_wide[ , c("Subject", "pass")], by = "Subject", all.x=TRUE)
data$pass
data = subset(data, pass == 1)
length(unique(data$Subject))
## R code for FOR PROBA LEARNING TASK OBIWAN
# last modified on April 2020 by David MUNOZ TORD
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE))
# PRELIMINARY STUFF ----------------------------------------
if(!require(pacman)) {
install.packages("pacman", "devtools")
library(devtools)
library(pacman)
}
pacman::p_load(tidyverse, plyr,dplyr,readr, car, BayesFactor, sjmisc, parallel, effectsize, pracma) #whatchout to have tidyBF 0.3.0
options(mc.cores = parallel::detectCores()) #to mulithread
# SETUP ------------------------------------------------------------------
task = 'PBlearning'
# Set working directory #change here if the switchdrive is not on your home folder
analysis_path <- file.path('~/Desktop/SwitchDrive/OBIWAN/PROBA_LEARNING/Analysis')
setwd(analysis_path)
# open dataset
full <- read_csv("~/Desktop/SwitchDrive/OBIWAN/PROBA_LEARNING/PBLearning.csv")
#load("~/OBIWAN/DERIVATIVES/BEHAV/PBL_OBIWAN_T0.RData") # if you dont want ot recompute and go directly to stats
# Preprocess --------------------------------------------------------------
data  <- subset(full, Session == 1) #subset #only session one
data  <- subset(data, Phase == 'proc1') #only learning phase
#factorize and rename
data$type = as.factor(revalue(data$imcor, c(A="AB", C="CD", E="EF")))
data$reward = revalue(data$feedback, c(Negatif=0, Positif=1))
data$side = revalue(data$Stim.RESP, c(x='L', n='R'))
data$subjID = data$Subject
#This loop is there to transform everything into one column "choice"
#this column takes a 1 if the action was to choose either A, C or E
#and takes 0 if the response is either B, D or F (and that independently of the side)
data$choice = c(1:length(data$Trial)) #initialize variable
for (i in  1:length(data$Trial)) {
if((data$side[i] == 'L')&(data$img[i] == 'A' || data$img[i] == 'C' || data$img[i] == 'E')) {
data$choice[i] = 1
} else if ((data$side[1] == 'R')&(data$imd[i] == 'A' || data$imd[i] == 'C' || data$imd[i] == 'E')) {
data$choice[i] = 1}
else {
data$choice[i] = 0}
}
data$reward = as.numeric(data$reward)
data$type = revalue(data$type, c(AB=12, CD=34, EF=56))
data$type = as.numeric(as.character(data$type))
bs = ddply(data, .(Subject, imcor), summarise, acc = mean(Stim.ACC, na.rm = TRUE))
# Crtierium chose A at 65%, C at 60% and E at 50% and min 30 trials.
bs_wide <- spread(bs, imcor, acc)
bs_wide$pass = c(1:length(bs_wide$Subject)) #initialize variable
for (i in  1:length(bs_wide$Subject)) {
if((bs_wide$A[i] >= 0.65) && (bs_wide$C[i] >=  0.60) && (bs_wide$E[i] >= 0.50 ))
{bs_wide$pass[i] = 1}
else {bs_wide$pass[i] = 0}
}
data = merge(data, bs_wide[ , c("Subject", "pass")], by = "Subject", all.x=TRUE)
length(unique(data$Subject))
data = subset(data, pass == 1)
length(unique(data$Subject))
citation()
sessionInfo()
citation("BayesFactor")
1.342716e-07
1.342716e-01\
1.342716e-01
## R code for FOR PROBA LEARNING TASK OBIWAN
# last modified on April 2020 by David MUNOZ TORD
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE))
# PRELIMINARY STUFF ----------------------------------------
if(!require(pacman)) {
install.packages("pacman", "devtools")
library(devtools)
library(pacman)
}
pacman::p_load(tidyverse, plyr,dplyr,readr, car, BayesFactor, sjmisc, parallel, effectsize, pracma) #whatchout to have tidyBF 0.3.0
options(mc.cores = parallel::detectCores()) #to mulithread
# SETUP ------------------------------------------------------------------
task = 'PBlearning'
# Set working directory #change here if the switchdrive is not on your home folder
analysis_path <- file.path('~/Desktop/SwitchDrive/OBIWAN/PROBA_LEARNING/Analysis')
setwd(analysis_path)
# open dataset
full <- read_csv("~/Desktop/SwitchDrive/OBIWAN/PROBA_LEARNING/PBLearning.csv")
#load("~/OBIWAN/DERIVATIVES/BEHAV/PBL_OBIWAN_T0.RData") # if you dont want ot recompute and go directly to stats
# Preprocess --------------------------------------------------------------
data  <- subset(full, Session == 1) #subset #only session one
data  <- subset(data, Phase == 'proc1') #only learning phase
#factorize and rename
data$type = as.factor(revalue(data$imcor, c(A="AB", C="CD", E="EF")))
data$reward = revalue(data$feedback, c(Negatif=0, Positif=1))
data$side = revalue(data$Stim.RESP, c(x='L', n='R'))
data$subjID = data$Subject
#This loop is there to transform everything into one column "choice"
#this column takes a 1 if the action was to choose either A, C or E
#and takes 0 if the response is either B, D or F (and that independently of the side)
data$choice = c(1:length(data$Trial)) #initialize variable
for (i in  1:length(data$Trial)) {
if((data$side[i] == 'L')&(data$img[i] == 'A' || data$img[i] == 'C' || data$img[i] == 'E')) {
data$choice[i] = 1
} else if ((data$side[1] == 'R')&(data$imd[i] == 'A' || data$imd[i] == 'C' || data$imd[i] == 'E')) {
data$choice[i] = 1}
else {
data$choice[i] = 0}
}
data$reward = as.numeric(data$reward)
data$type = revalue(data$type, c(AB=12, CD=34, EF=56))
data$type = as.numeric(as.character(data$type))
bs = ddply(data, .(Subject, imcor), summarise, acc = mean(Stim.ACC, na.rm = TRUE))
# Crtierium chose A at 65%, C at 60% and E at 50% and min 30 trials.
bs_wide <- spread(bs, imcor, acc)
bs_wide$pass = c(1:length(bs_wide$Subject)) #initialize variable
for (i in  1:length(bs_wide$Subject)) {
if((bs_wide$A[i] >= 0.65) && (bs_wide$C[i] >=  0.60) && (bs_wide$E[i] >= 0.50 ))
{bs_wide$pass[i] = 1}
else {bs_wide$pass[i] = 0}
}
data = merge(data, bs_wide[ , c("Subject", "pass")], by = "Subject", all.x=TRUE)
length(unique(data$Subject))
data = subset(data, pass == 1)
length(unique(data$Subject))
dataclean <- select(data, c(subjID, type, choice, reward, Group))
#check that nobody has more than 200 trial
count_trial = dataclean %>% group_by(subjID) %>%tally()
# Estimate model's parameters and compare model's fit ----------------------------------------------------------
set.seed(666)
Nrep = 500;
source('~/Desktop/SwitchDrive/OBIWAN/PROBA_LEARNING/Analysis/PST_Q_learning.R', echo=TRUE)
# model1 : RW Q-Learning with one learning rate and a softmax inverse temperature -----------------------------------------------------------------
k1 = 2
subj = unique(dataclean$subjID)
alpha = c(); beta = c(); nll= c(); ID = c(); group = c(); trials = c()
LB = c(0, 0); UB = c(1, 10) # parameters lower and upper bounds
for (s in subj) {
data = subset(dataclean, subjID == s)
param_rep = c(); nll_rep = c()
for (i in  1:Nrep) {
x0 = c(rand(), 10*rand()); #different parameter initial values to avoid local minima
f = fmincon(x0=x0,fn=PST_q, data = data, lb = LB, ub = UB) #optimize
param_rep = rbind(param_rep, f$par); nll_rep = rbind(nll_rep, f$value)
}
pos = which.min(nll_rep)
alpha = rbind(alpha,param_rep[pos,1]); beta = rbind(beta,param_rep[pos,2]); nll = rbind(nll,nll_rep[pos]); ID = rbind(ID,s); group = rbind(group, ifelse(s > 199, 'obese', 'lean')); trials = rbind(trials,length(data$subjID))
print(paste('done subj', s))
}
df1 = cbind(ID, alpha, beta, nll, group, trials)
colnames(df1) = c('ID', 'alpha', 'beta', 'nll', 'group', 'trial');
df1 = as_tibble(df1); df1$group = as.factor(df1$group); df1$ID =as.factor(df1$ID)
df1[] <- lapply(df1, function(x) {if(is.character(x)) as.numeric(as.character(x)) else x})
# model2 : RW Q-Learning with dual learning rate (gain and loss) and  softmax inverse temperature -----------------------------------------------------------------
k2 = 3 # number of free parameteres
subj = unique(dataclean$subjID)
alphaG = c(); alphaL = c(); beta = c(); nll= c(); ID = c(); group = c(); trials = c()
LB = c(0, 0, 0); UB = c(1, 1, 10) # parameters lower and upper bounds
for (s in subj) {
data = subset(dataclean, subjID == s)
param_rep = c(); nll_rep = c()
for (i in  1:Nrep) {
x0 = c(rand(), rand(), 10*rand()); #different parameter initial values to avoid local minima
f = fmincon(x0=x0,fn=PST_q_dual, data = data, lb = LB, ub = UB) #optimize
param_rep = rbind(param_rep, f$par); nll_rep = rbind(nll_rep, f$value)
}
pos = which.min(nll_rep)
alphaG = rbind(alphaG,param_rep[pos,1]); alphaL = rbind(alphaL,param_rep[pos,2]); beta = rbind(beta,param_rep[pos,3]); nll = rbind(nll,nll_rep[pos]); ID = rbind(ID,s); group = rbind(group, ifelse(s > 200, 'obese', 'lean')); trials = rbind(trials,length(data$subjID))
print(paste('done subj', s))
}
df2 = cbind(ID, alphaG, alphaL, beta, nll, group, trials)
colnames(df2) = c('ID', 'alpha_gain', 'alpha_loss','beta', 'nll', 'group', 'trials');
df2 = as_tibble(df2); df2$group =as.factor(df2$group); df2$ID =as.factor(df2$ID)
df2[] <- lapply(df2, function(x) {if(is.character(x)) as.numeric(as.character(x)) else x})
# Bayesian model comparison for group studies ---------------------------------------------------------------- see Stephan et al., 2009
df1$BIC =  k1*log(df1$trial) -2*(-df1$nll) #Bayesian information criterion for each subj
df2$BIC = k2*log(df1$trial) -2*(-df2$nll) #Bayesian information criterion for each subj
model_BIC = cbind(df1$BIC, df2$BIC)
source('~/Desktop/SwitchDrive/OBIWAN/PROBA_LEARNING/Analysis/VBA.R', echo=F)
#Variational Bayesian Analysis (Daunizeau & Rigoux)
#a list with the posterior estimates of the Dirichlet parameters (alpha), the expected model frequencies (r), the exceedance probabilities (xp), the Bayesian Omnibus Risk (bor), and the protected exceedance probabilities (pxp).
VBA = VB_bms(-model_BIC/2) ; VBA  #model 1 is "better"
# $alpha
# [1] 59.702531  2.297469
#
# $r
# [1] 0.96294405 0.03705595
#
# $xp
# [1] 1 0
#
# $bor
# [1] 1.342716e-07
#
# $pxp
# [1] 9.999999e-01 6.713580e-08
df = df1
save(df, file="~/Desktop/SwitchDrive/OBIWAN/PROBA_LEARNING/PBL_OBIWAN_T0.RData")
# also check model fit ---------------------------------------------------------------
BIC1 = k1*log(length(df1$ID)) -2*mean(-df1$nll); BIC1 # Bayesian information criterion k*ln(n) - 2*logLik
AIC1 = 2*k1 - 2*mean(-df1$nll); AIC1 #AIC=2k-2*logLik
BIC2 = k2*log(length(df2$ID)) -2*mean(-df2$nll); BIC2 # Bayesian information criterion k*ln(n) - 2*logLik
AIC2 = 2*k2 - 2*mean(-df2$nll); AIC2 #AIC=2k-2*logLik
62.01033 -57.82164
62.01033- 64.40028
57.82164 - 58.11724
